{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOBaYMe6daeLGTWO4s1kPk1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### Data Wrangling"],"metadata":{"id":"PbvHKaxEybqP"}},{"cell_type":"code","source":["from google.colab import drive\n","\n","drive.mount('/content/gdrive', force_remount=True)\n","\n","root_dir = \"/content/gdrive/My Drive/NLP Sentiment Analysis\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TytpVVLzcVO8","executionInfo":{"status":"ok","timestamp":1670472275928,"user_tz":360,"elapsed":4809,"user":{"displayName":"Josiah Coad","userId":"12096942713822516722"}},"outputId":"1da954eb-97fe-434a-f466-7b34c4e54356"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","source":["with open(f'{root_dir}/train_neg_reviews.txt') as f:\n","  contents = f.read()\n","  train_neg_reviews = [review[len('4\\t'):] for review in contents.split('\\n')]\n","\n","with open(f'{root_dir}/train_pos_reviews.txt') as f:\n","  contents = f.read()\n","  train_pos_reviews = [review[len('4\\t'):] for review in contents.split('\\n')]\n","\n","with open(f'{root_dir}/test_neg_reviews.txt') as f:\n","  contents = f.read()\n","  test_neg_reviews = [review[len('4\\t'):] for review in contents.split('\\n')]\n","\n","with open(f'{root_dir}/test_pos_reviews.txt') as f:\n","  contents = f.read()\n","  test_pos_reviews = [review[len('4\\t'):] for review in contents.split('\\n')]"],"metadata":{"id":"Un9RO-iWbD-8","executionInfo":{"status":"ok","timestamp":1670472276106,"user_tz":360,"elapsed":182,"user":{"displayName":"Josiah Coad","userId":"12096942713822516722"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["train_docs = train_neg_reviews + train_pos_reviews\n","y_train = [0]*len(train_neg_reviews) + [1]*len(train_pos_reviews)"],"metadata":{"id":"PuxLXTMgfDvC","executionInfo":{"status":"ok","timestamp":1670472276107,"user_tz":360,"elapsed":3,"user":{"displayName":"Josiah Coad","userId":"12096942713822516722"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","vectorizer = TfidfVectorizer()\n","vectorizer.fit(train_docs)\n","X_train = vectorizer.transform(train_docs)"],"metadata":{"id":"K9kCWCbdfniM","executionInfo":{"status":"ok","timestamp":1670472287057,"user_tz":360,"elapsed":10952,"user":{"displayName":"Josiah Coad","userId":"12096942713822516722"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["test_docs = test_neg_reviews + test_pos_reviews\n","y_test = [0]*len(test_neg_reviews) + [1]*len(test_pos_reviews)\n","X_test = vectorizer.transform(test_docs)\n","\n","X = vectorizer.transform(train_docs+test_docs)\n","y = y_train + y_test"],"metadata":{"id":"LqsT72ZvgMco","executionInfo":{"status":"ok","timestamp":1670472297972,"user_tz":360,"elapsed":10918,"user":{"displayName":"Josiah Coad","userId":"12096942713822516722"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["A random forest is a type of ensemble machine learning model that is made up of multiple decision trees. Ensemble models combine the predictions of multiple individual models to make more accurate predictions. In a random forest, each decision tree is trained on a random subset of the data, and the final prediction is made by averaging the predictions of all the individual decision trees.\n","\n","Here is an example of how to train a random forest using the scikit-learn library in Python:\n"],"metadata":{"id":"vTKPXVGWf-Es"}},{"cell_type":"markdown","source":["## Model Fitting"],"metadata":{"id":"hhFWIkRCyhRN"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestClassifier\n","\n","# Create a random forest classifier with 100 trees\n","model = RandomForestClassifier()\n","\n","# Train the model on training data\n","model.fit(X_train, y_train)\n","\n","# Score\n","model.score(X_test, y_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V-duzIGdfz_O","executionInfo":{"status":"ok","timestamp":1670472330497,"user_tz":360,"elapsed":32528,"user":{"displayName":"Josiah Coad","userId":"12096942713822516722"}},"outputId":"4725177a-a28b-43b1-c818-1949dcdf0e25"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.826133909287257"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["# get feature (word) importances\n","print(model.feature_importances_)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zhSlIap54YSN","executionInfo":{"status":"ok","timestamp":1670472346376,"user_tz":360,"elapsed":826,"user":{"displayName":"Josiah Coad","userId":"12096942713822516722"}},"outputId":"7947c20f-2197-44af-f9f8-017498944587"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["[3.25983162e-05 5.70025769e-05 7.50043823e-06 ... 0.00000000e+00\n"," 0.00000000e+00 0.00000000e+00]\n"]}]},{"cell_type":"markdown","source":["Grid search can help us find the best parameters"],"metadata":{"id":"Dcsq87hTzGVd"}},{"cell_type":"code","source":["from sklearn.model_selection import GridSearchCV\n","\n","# Define the hyperparameter grid for the model\n","param_grid = {\n","    'n_estimators': [10, 100, 1000],\n","    'max_depth': [5, 10, 50, 100],\n","    'min_impurity_decrease': [0, 0.1, 1],\n","    'max_features': [1, 10, 100, 1000, None]\n","}\n","\n","model_grid = RandomForestClassifier()\n","\n","# Use GridSearchCV to search for the best hyperparameters\n","clf = GridSearchCV(model_grid, param_grid, cv=5)\n","\n","\n","clf.fit(X, y)\n","\n","# Print the best hyperparameters\n","print(f\"Best hyperparameters: {clf.best_params_}. Score: {clf.best_score_:.2f}\")"],"metadata":{"id":"Hx8AcFm1kyaC","executionInfo":{"status":"aborted","timestamp":1670468658622,"user_tz":360,"elapsed":5,"user":{"displayName":"Josiah Coad","userId":"12096942713822516722"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Import the necessary libraries\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.datasets import make_classification\n","\n","# Create a gradient boosting classifier\n","clf = GradientBoostingClassifier()\n","\n","# Train the classifier on the data\n","clf.fit(X_train, y_train)\n","\n","# Make predictions on new data\n","clf.score(X_test, y_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xb0XoZpbgp5f","executionInfo":{"status":"ok","timestamp":1670465813878,"user_tz":360,"elapsed":67213,"user":{"displayName":"Josiah Coad","userId":"12096942713822516722"}},"outputId":"2edbf1f0-b7f0-4a77-c2b3-fc4c41bdc71c"},"execution_count":54,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8095352371810255"]},"metadata":{},"execution_count":54}]},{"cell_type":"code","source":["import xgboost as xgb\n","\n","# Create the XGBoost model\n","model = xgb.XGBClassifier()\n","\n","# Train the model on the training data\n","model.fit(X_train, y_train)\n","\n","# Evaluate the model on the test data\n","accuracy = model.score(X_test, y_test)\n","print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"],"metadata":{"id":"AUb8MfvjyxIm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Contextual Polarity"],"metadata":{"id":"p9IMavwz9gt-"}},{"cell_type":"code","source":["from sklearn.linear_model import LogisticRegression\n","import numpy as np\n","model = LogisticRegression()\n","model.fit(X_train, y_train)\n","feature_names = np.array(vectorizer.get_feature_names())\n","sorted_coef_index = model.coef_[0].argsort()\n","print(\"Negative Words\", feature_names[sorted_coef_index[:10]])\n","print(\"Positive Words\", feature_names[sorted_coef_index[-10:]])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VmLoGsk28vMk","executionInfo":{"status":"ok","timestamp":1670473176163,"user_tz":360,"elapsed":1054,"user":{"displayName":"Josiah Coad","userId":"12096942713822516722"}},"outputId":"4d549b54-87e5-4f62-f6bb-d773b2e6bd2a"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Negative Words ['bad' 'worst' 'waste' 'awful' 'boring' 'poor' 'no' 'terrible' 'just'\n"," 'even']\n","Positive Words ['amazing' 'and' 'love' 'fun' 'well' 'wonderful' 'perfect' 'excellent'\n"," 'best' 'great']\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from matplotlib.colors import ListedColormap\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.datasets import make_moons, make_circles, make_classification\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.svm import SVC\n","from sklearn.gaussian_process import GaussianProcessClassifier\n","from sklearn.gaussian_process.kernels import RBF\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n","\n","names = [\n","    \"Nearest Neighbors\",\n","    \"Linear SVM\",\n","    \"RBF SVM\",\n","    \"Gaussian Process\",\n","    \"Decision Tree\",\n","    \"Random Forest\",\n","    \"Neural Net\",\n","    \"AdaBoost\",\n","    \"Naive Bayes\",\n","    \"QDA\",\n","]\n","\n","classifiers = [\n","    KNeighborsClassifier(30),\n","    SVC(kernel=\"linear\", C=0.025),\n","    SVC(gamma=2, C=1),\n","    GaussianProcessClassifier(1.0 * RBF(1.0)),\n","    DecisionTreeClassifier(max_depth=50),\n","    RandomForestClassifier(max_depth=50, n_estimators=100, max_features=10),\n","    MLPClassifier(alpha=1, max_iter=1000),\n","    AdaBoostClassifier(),\n","    GaussianNB(),\n","    QuadraticDiscriminantAnalysis(),\n","]\n","\n","\n","figure = plt.figure(figsize=(27, 9))\n","i = 1\n","# preprocess dataset, split into training and test part\n","\n","X_normal = StandardScaler(with_mean=False).fit_transform(X)\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.4, random_state=42\n",")\n","\n","x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n","y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n","\n","scores = {}\n","# iterate over classifiers\n","for name, clf in zip(names, classifiers):\n","    clf.fit(X_train, y_train)\n","    score = clf.score(X_test, y_test)\n","    print(name, score)\n","    scores[name] = score"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"06O9ABt9opJn","outputId":"60a45b63-3dd9-4181-82ea-179320a0633b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Nearest Neighbors 0.7341687774963338\n","Linear SVM 0.8110918544194108\n"]}]},{"cell_type":"code","source":["# doc2vec embeddings (instead of TF-IDF)\n","from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n","\n","# Create a list of TaggedDocument objects\n","documents = [TaggedDocument(doc.split(), [i]) for i, doc in enumerate(train_docs)]\n","\n","# Initialize the Doc2Vec model\n","model = Doc2Vec(vector_size=300, min_count=1, epochs=50)\n","\n","# Build the vocabulary\n","model.build_vocab(documents)\n","\n","# Train the model\n","model.train(documents, total_examples=model.corpus_count, epochs=model.epochs)\n","\n","# Generate vector representation for a document\n","X_train = [model.infer_vector(doc) for doc in train_docs]"],"metadata":{"id":"YMyHfdHz4QR_"},"execution_count":null,"outputs":[]}]}